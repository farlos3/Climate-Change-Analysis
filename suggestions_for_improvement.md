# üöÄ ‡∏Ç‡πâ‡∏≠‡πÄ‡∏™‡∏ô‡∏∞‡πÅ‡∏ô‡∏∞‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• Climate Forecasting

## üìà Features ‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à

### 1. **Temperature Volatility Features**
```python
# ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏±‡∏ô‡∏ú‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥
df['t2m_volatility_3d'] = df['t2m'].rolling(3).std() / df['t2m'].rolling(3).mean()
df['t2m_volatility_7d'] = df['t2m'].rolling(7).std() / df['t2m'].rolling(7).mean()

# ‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÅ‡∏õ‡∏•‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ß‡∏±‡∏ô
df['t2m_daily_change'] = df['t2m'].diff()
df['t2m_acceleration'] = df['t2m_daily_change'].diff()
```

### 2. **Weather Pattern Features**
```python
# Heating/Cooling degree days
df['hdd'] = np.maximum(18 - df['t2m'], 0)  # Heating Degree Days
df['cdd'] = np.maximum(df['t2m'] - 18, 0)  # Cooling Degree Days

# Temperature anomalies
df['t2m_monthly_avg'] = df.groupby('month')['t2m'].transform('mean')
df['t2m_anomaly'] = df['t2m'] - df['t2m_monthly_avg']
```

### 3. **Cross-variable Interactions**
```python
# ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ä‡∏±‡πâ‡∏ô‡∏≠‡∏∏‡∏ì‡∏´‡∏†‡∏π‡∏°‡∏¥
df['temp_gradient_surface'] = df['t2m'] - df['ts']
df['temp_gradient_soil'] = df['tsoil1'] - df['tsoil4']

# Wind-temperature interaction
df['wind_chill_factor'] = df['t2m'] - (df['v10m'] * 0.1)  # simplified wind chill
```

## ü§ñ Model Architecture Recommendations

### LSTM ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 16,000+ rows

**‚úÖ ‡πÉ‡∏ä‡πâ‡πÑ‡∏î‡πâ‡πÅ‡∏ô‡πà‡∏ô‡∏≠‡∏ô!** ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Ç‡πâ‡∏≠‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥:

#### **Model Configuration:**
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization

# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 16K+ samples
model = Sequential([
    LSTM(128, return_sequences=True, input_shape=(seq_length, n_features)),
    BatchNormalization(),
    Dropout(0.2),
    
    LSTM(64, return_sequences=False),
    BatchNormalization(), 
    Dropout(0.2),
    
    Dense(128, activation='relu'),
    Dropout(0.1),
    
    Dense(7)  # 7-day forecast outputs
])

# Memory efficient training
batch_size = 32  # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ 32
epochs = 50-100
```

#### **Sequence Preparation:**
```python
# ‡πÉ‡∏ä‡πâ 14-21 ‡∏ß‡∏±‡∏ô‡∏¢‡πâ‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏á‡πÄ‡∏õ‡πá‡∏ô sequence
seq_length = 14  # ‡∏´‡∏£‡∏∑‡∏≠ 21 ‡∏ß‡∏±‡∏ô
n_features = len(feature_cols)

def create_sequences(X, y, seq_length):
    X_seq, y_seq = [], []
    for i in range(seq_length, len(X)):
        X_seq.append(X[i-seq_length:i])
        y_seq.append(y[i])
    return np.array(X_seq), np.array(y_seq)
```

### **Hybrid Architecture** (‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥!)
```python
# ‡∏£‡∏ß‡∏° LSTM + LightGBM
# 1. ‡πÉ‡∏ä‡πâ LSTM ‡∏™‡∏Å‡∏±‡∏î temporal features
# 2. ‡πÉ‡∏ä‡πâ LightGBM ‡∏ó‡∏≥ final prediction

# LSTM encoder
lstm_features = lstm_model.predict(X_sequences)

# Combine ‡∏Å‡∏±‡∏ö traditional features
combined_features = np.hstack([traditional_features, lstm_features])

# Final prediction ‡∏î‡πâ‡∏ß‡∏¢ LightGBM
final_model = MultiOutputRegressor(LGBMRegressor(**best_params))
final_model.fit(combined_features, y)
```

## ‚ö° Optimization Strategies

### 1. **Ensemble Methods**
```python
# ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢
predictions_lgbm = model_lgbm.predict(X_test)
predictions_lstm = model_lstm.predict(X_test_seq)

# Weighted ensemble
ensemble_pred = 0.6 * predictions_lgbm + 0.4 * predictions_lstm
```

### 2. **Advanced Feature Selection**
```python
# Feature importance ‡∏à‡∏≤‡∏Å SHAP
import shap
explainer = shap.TreeExplainer(model_lgbm)
shap_values = explainer.shap_values(X_train)

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å top features
feature_importance = np.abs(shap_values).mean(0)
top_features = feature_importance.argsort()[-200:]  # top 200 features
```

## üéØ ‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡∏∏‡∏ì

### **Priority 1: ‡∏ó‡∏î‡∏•‡∏≠‡∏á Hybrid Model**
1. ‡πÉ‡∏ä‡πâ LightGBM ‡∏õ‡∏±‡∏à‡∏à‡∏∏‡∏ö‡∏±‡∏ô‡πÄ‡∏õ‡πá‡∏ô baseline
2. ‡∏™‡∏£‡πâ‡∏≤‡∏á LSTM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö temporal patterns
3. ‡∏£‡∏ß‡∏° predictions ‡∏î‡πâ‡∏ß‡∏¢ ensemble

### **Priority 2: Advanced Feature Engineering**
- Temperature volatility features
- Weather pattern indices
- Cross-variable interactions

### **Priority 3: Model Optimization**
- Early stopping ‡∏Å‡∏±‡∏ö validation set
- Cross-validation for robust evaluation
- Hyperparameter tuning for LSTM

## üìä Expected Improvements

‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏™‡∏ö‡∏Å‡∏≤‡∏£‡∏ì‡πå:
- **RMSE Day +1**: 0.659 ‚Üí 0.55-0.60 (-15-20%)
- **RMSE Day +7**: 1.207 ‚Üí 1.05-1.15 (-10-15%)
- **Overall RMSE**: 1.067 ‚Üí 0.95-1.00 (-10-15%)

## ‚ö†Ô∏è ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏£‡∏£‡∏∞‡∏ß‡∏±‡∏á

1. **LSTM Memory Usage**: monitor ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ RAM
2. **Training Time**: LSTM ‡πÉ‡∏ä‡πâ‡πÄ‡∏ß‡∏•‡∏≤‡∏ô‡∏≤‡∏ô‡∏Å‡∏ß‡πà‡∏≤ LightGBM ‡∏°‡∏≤‡∏Å
3. **Overfitting**: ‡πÉ‡∏ä‡πâ validation set ‡πÅ‡∏•‡∏∞ early stopping
4. **Data Leakage**: ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ features ‡πÑ‡∏°‡πà leak ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• future

‡∏°‡∏µ‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ code implementation ‡∏™‡πà‡∏ß‡∏ô‡πÑ‡∏´‡∏ô‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?